{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System : Python  2.7.12 | packaged by conda-forge | (default, Sep  8 2016, 14:22:31) \n",
      "[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] \n",
      "\n",
      "\n",
      "Directory Structure:\n",
      "/home/ubuntu/ana_py27_p2/projects/CervicalCancer\n",
      ".\n",
      "├── unused_utils\n",
      "│   ├── __init__.py\n",
      "│   ├── __init__.pyc\n",
      "│   ├── utils.pyc\n",
      "│   └── unused_utils.py\n",
      "├── __init__.py\n",
      "├── 001 - Initial Setup.ipynb\n",
      "├── 002 - Data Setup & General Notes.ipynb\n",
      "├── 003 - Baseline.ipynb\n",
      "├── Trash\n",
      "│   ├── 002-01 - Train Data (only) Setup.ipynb\n",
      "│   └── Base CNN Model Train Data with Preprocessing.ipynb\n",
      "├── 004-1 - Base CNN Model - Sample.ipynb\n",
      "├── 004-2 - Base CNN Model - Train Data.ipynb\n",
      "├── 005 - Exploratoring the Data.ipynb\n",
      "├── 006 - Non Neural Net Classifiers.ipynb\n",
      "├── 007 - Gradient Boosting Classifier.ipynb\n",
      "├── data\n",
      "│   ├── additional_Type_1_v2.7z\n",
      "│   ├── additional_Type_3_v2.7z\n",
      "│   ├── additional_Type_2_v2.7z\n",
      "│   ├── train\n",
      "│   │   ├── Type_1 [168 entries exceeds filelimit, not opening dir]\n",
      "│   │   ├── Type_2 [524 entries exceeds filelimit, not opening dir]\n",
      "│   │   └── Type_3 [302 entries exceeds filelimit, not opening dir]\n",
      "│   ├── test\n",
      "│   │   └── unknown [512 entries exceeds filelimit, not opening dir]\n",
      "│   ├── preview\n",
      "│   ├── downloads\n",
      "│   │   ├── sample_submission.csv.zip\n",
      "│   │   ├── test.7z\n",
      "│   │   └── train.7z\n",
      "│   ├── valid\n",
      "│   │   ├── Type_1 [82 entries exceeds filelimit, not opening dir]\n",
      "│   │   ├── Type_2 [257 entries exceeds filelimit, not opening dir]\n",
      "│   │   └── Type_3 [148 entries exceeds filelimit, not opening dir]\n",
      "│   ├── sample\n",
      "│   │   ├── preview\n",
      "│   │   ├── train\n",
      "│   │   │   ├── Type_1 [80 entries exceeds filelimit, not opening dir]\n",
      "│   │   │   ├── Type_2 [249 entries exceeds filelimit, not opening dir]\n",
      "│   │   │   └── Type_3 [144 entries exceeds filelimit, not opening dir]\n",
      "│   │   ├── valid\n",
      "│   │   │   ├── Type_1 [61 entries exceeds filelimit, not opening dir]\n",
      "│   │   │   ├── Type_2 [191 entries exceeds filelimit, not opening dir]\n",
      "│   │   │   └── Type_3 [110 entries exceeds filelimit, not opening dir]\n",
      "│   │   ├── test\n",
      "│   │   │   └── unknown [201 entries exceeds filelimit, not opening dir]\n",
      "│   │   └── weights\n",
      "│   │       ├── weights-improvement-00-0.22.hdf5\n",
      "│   │       ├── weights-improvement-01-0.35.hdf5\n",
      "│   │       ├── weights-improvement-03-0.36.hdf5\n",
      "│   │       ├── weights-improvement-06-0.45.hdf5\n",
      "│   │       ├── weights-improvement-07-0.47.hdf5\n",
      "│   │       ├── BaseCNN35EpochRun_1.hdf5\n",
      "│   │       ├── weights2-improvement-00-0.30.hdf5\n",
      "│   │       ├── weights2-improvement-01-0.37.hdf5\n",
      "│   │       ├── weights2-improvement-05-0.43.hdf5\n",
      "│   │       ├── weights2-improvement-11-0.46.hdf5\n",
      "│   │       └── weights2-improvement-12-0.50.hdf5\n",
      "│   ├── submissions\n",
      "│   │   ├── sample_submission_000.csv\n",
      "│   │   ├── sub_001.csv\n",
      "│   │   ├── sub_002.csv\n",
      "│   │   ├── sub_003.csv\n",
      "│   │   ├── sub_004.csv\n",
      "│   │   └── sub_005.csv\n",
      "│   ├── weights [29 entries exceeds filelimit, not opening dir]\n",
      "│   └── models\n",
      "│       └── VGG16D_1.yaml\n",
      "├── Transfer Learning Tests 2.ipynb\n",
      "├── Transfer Learning Tests 1.ipynb\n",
      "├── Base CNN Model Train Data with Augmentation.ipynb\n",
      "└── Transfer Learning Tests 4.ipynb\n",
      "\n",
      "31 directories, 43 files\n",
      "\n",
      "\n",
      "Keras version: 2.0.2 , backend: theano , image_format: channels_last\n",
      "\n",
      "\n",
      "Environment : \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='../../ana_py27_p2.yml' target='_blank'>../../ana_py27_p2.yml</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/ana_py27_p2/ana_py27_p2.yml"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project Template Import Cell\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from inspect import getsourcefile\n",
    "\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Standard Notebook Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "CURR_PATH = os.path.abspath(getsourcefile(lambda:0)).rsplit('/', 1)[0] # Get filepath of this notebook\n",
    "module_path = os.path.join(os.path.dirname(CURR_PATH), 'utils') # Make module path for one dir up and one down into utils\n",
    "if module_path not in sys.path: # Append to system path list\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import utils as utils ; reload(utils)\n",
    "\n",
    "print('System : Python ', os.sys.version , '\\n\\n')\n",
    "\n",
    "print('Directory Structure:')\n",
    "print(CURR_PATH)\n",
    "!tree -cn --filelimit 16\n",
    "\n",
    "# Keras Setup\n",
    "import keras\n",
    "print('\\n\\nKeras version:' , keras.__version__ ,\n",
    "      ', backend:' , keras.backend.backend(),\n",
    "      ', image_format:' , keras.backend.image_data_format())\n",
    "\n",
    "random_seed = 2\n",
    "\n",
    "print('\\n\\nEnvironment : ')\n",
    "FileLink('../../ana_py27_p2.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Strategy\n",
    "[Fine-tuning the top layers of a a pre-trained network](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) - I will attempt to follow the Final Part of this tutorial, but the final part build upon work done in Part 2, so I will start at Part 2 - Using the bottleneck features of a pre-trained network: 90% accuracy in a minute\n",
    "\n",
    "1. Load up VGG16 and its weights\n",
    "2. Chop off the top layer of VGG16, and add a small fully connected model\n",
    "3. Freeze all the layers in VGG16, and train the final fully connected model\n",
    "4. Unfreeze the last convolutional layer also, and retrain the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A fix for truncated image error\n",
    "# http://stackoverflow.com/questions/12984426/python-pil-ioerror-image-file-truncated-with-big-images\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# DIRECTORIES\n",
    "# !!!SAMPLE\n",
    "train_data_dir = 'data/sample/train' ; validation_data_dir = 'data/sample/valid' ; test_data_dir = 'data/sample/test'\n",
    "weights_dir = 'data/sample/weights'\n",
    "\n",
    "# !!!TRAIN\n",
    "#train_data_dir = 'data/train' ; validation_data_dir = 'data/valid' ; test_data_dir = 'data/test'\n",
    "#weights_dir = 'data/weights'\n",
    "\n",
    "submission_dir = 'data/submissions'\n",
    "pt_models_dir = '../pretrained_models'\n",
    "models_dir = 'data/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473\n",
      "362\n",
      "8\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# DATA META-DATA\n",
    "# Get the DataSets and load into a DataFrame for easy reference\n",
    "\n",
    "img_width, img_height = 224, 224\n",
    "#img_width, img_height = 300, 300\n",
    "\n",
    "num_training_samples = 0\n",
    "for class_dir in utils.get_non_hidden_dir_contents(train_data_dir):\n",
    "    num_training_samples += len(utils.get_non_hidden_dir_contents(class_dir))\n",
    "num_validation_samples = 0\n",
    "for class_dir in utils.get_non_hidden_dir_contents(validation_data_dir):\n",
    "    num_validation_samples += len(utils.get_non_hidden_dir_contents(class_dir))    \n",
    "\n",
    "print(num_training_samples)\n",
    "print(num_validation_samples)\n",
    "\n",
    "# !!!SAMPLE\n",
    "epochs = 15\n",
    "batch_size=64\n",
    "\n",
    "# !!!TRAIN\n",
    "#epochs=30\n",
    "#batch_size=64\n",
    "\n",
    "input_shape = (img_width, img_height, 3)\n",
    "\n",
    "train_steps = int(math.ceil(num_training_samples / batch_size))\n",
    "validation_steps =  int(math.ceil(num_validation_samples / batch_size))\n",
    "print(train_steps)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load up VGG16D architecture & saved best weights from last model runs and Unfreeze the Dense and last Conv block\n",
    "\n",
    "from keras.models import model_from_yaml\n",
    "import yaml\n",
    "\n",
    "# Load Model VGG 16D - VGG16 + Dense top model\n",
    "yaml_file = open(os.path.join(models_dir, 'vgg16model_1.yaml'), 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "vgg16d_1 = model_from_yaml(loaded_model_yaml)\n",
    "\n",
    "vgg16d_1 = Model(inputs=[vgg16d_1.input], outputs=[vgg16d_1.layers[-5].output])\n",
    "\n",
    "# load best weights from previously trained model (Everything frozen apart from top Dense model)\n",
    "vgg16d_1.load_weights(os.path.join(weights_dir,'VGG16D_1_2-improvement-06-0.56.hdf5'))\n",
    "print(\"Loaded weights from disk\")\n",
    "#Make top Dense and top Convolution Block Trainable\n",
    "#for l in vgg16d_1.layers[:]:\n",
    "#    l.trainable = False\n",
    "for l in vgg16d_1.layers[14:]:\n",
    "    l.trainable = True\n",
    "vgg16d_1.summary() # Trainable parameters about 13,502,979 i.e 13 million compared to the original 138 million\n",
    "# We can actually only train the weights in the final convolution block, and in the final dense layer\n",
    "print(vgg16d_1.layers[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from disk\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               25690368  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 33,326,403.0\n",
      "Trainable params: 31,590,915.0\n",
      "Non-trainable params: 1,735,488.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load up VGG16 architecture & inspect\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Dropout\n",
    "from keras.models import model_from_yaml\n",
    "\n",
    "import yaml\n",
    "# Load the full vgg16 model\n",
    "yaml_file = open(os.path.join(pt_models_dir, 'vgg16model_1.yaml'), 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "vgg16d_1 = model_from_yaml(loaded_model_yaml)\n",
    "\n",
    "# Load the original vgg16 model weights\n",
    "vgg16d_1.load_weights(os.path.join(pt_models_dir,'vgg16weights_1.hdf5'))\n",
    "print(\"Loaded weights from disk\")\n",
    "\n",
    "#vgg16.summary()\n",
    "# Chop off the last 12 layers- the last dense block, the last 2 conv blocks\n",
    "#vgg16_p = Model(inputs=[vgg16.input], outputs=[vgg16.get_layer('block5_pool').output]) #Grab output by name\n",
    "vgg16d_1 = Model(inputs=[vgg16d_1.input], outputs=[vgg16d_1.layers[-9].output])\n",
    "#utils.show_ascii_model(vgg16_p)\n",
    "from keras import regularizers\n",
    "# Create a new Dense model to add to the end of vgg16\n",
    "x = Flatten()(vgg16d_1.layers[-1].output)\n",
    "x = Dense(256, activation='relu') (x)\n",
    "\n",
    "#x = Dense(256, activation='relu', kernel_regularizer= regularizers.l2(1.0)) (x)\n",
    "\n",
    "x = Dropout(0.7) (x)\n",
    "\n",
    "last = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# Add to the end of VGG16\n",
    "vgg16d_1 = Model(inputs=[vgg16d_1.input], outputs=[last])\n",
    "# load weights into new model\n",
    "\n",
    "\n",
    "#utils.show_ascii_model(vgg16_)\n",
    "#Freeze all the weights in the original vgg16 layers before compile\n",
    "for l in vgg16d_1.layers:\n",
    "    l.trainable=False\n",
    "for l in vgg16d_1.layers[10:]:\n",
    "    l.trainable = True\n",
    "vgg16d_1.summary() # Trainable parameters about 13,502,979 i.e 13 million compared to the original 138 million\n",
    "# We can actually only train the weights in the final convolution block, and in the final dense block\n",
    "#print(vgg16d_1.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "sgd = SGD(lr=1e-6, momentum=0.9) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg16d_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://github.com/Lasagne/Recipes/issues/20\n",
    "# https://stats.stackexchange.com/questions/217481/how-to-normalize-data-for-vgg-16-pretrained-model\n",
    "def preprocess(img):\n",
    "    \"\"\"\n",
    "    I don't get this - I thought this took an individual image\n",
    "    I am missing a dimension somewhere\n",
    "    Does it take a batch of images?? - I don't think so according to docs\n",
    "    ???????????????????\n",
    "    \"\"\"\n",
    "    #print(img.shape) #(224, 224, 3)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    #print(img.shape) #(1, 224, 224, 3)\n",
    "    # 'RGB'->'BGR'\n",
    "    img = img[:, :, :, ::-1]\n",
    "    # Zero-center by mean pixel\n",
    "    img[:, :, :, 0] -= 103.939\n",
    "    img[:, :, :, 1] -= 116.779\n",
    "    img[:, :, :, 2] -= 123.68\n",
    "    #img[:, :, :, 0] -= 78.939\n",
    "    #img[:, :, :, 1] -= 91.779\n",
    "    #img[:, :, :, 2] -= 148.68\n",
    "    #print(img.shape)\n",
    "    #print(img)\n",
    "    return img\n",
    "\n",
    "import cv2\n",
    "import PIL\n",
    "#import PIL.Image\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "def preprocess_pk(img):\n",
    "    \"\"\"\n",
    "    I don't get this - I thought this took an individual image\n",
    "    I am missing a dimension somewhere\n",
    "    Does it take a batch of images?? - I don't think so according to docs\n",
    "    ???????????????????\n",
    "    \"\"\"\n",
    "    # Set parameters\n",
    "    min_params = (130,100,150) ; max_params = (180,256,256)\n",
    "    # Convert back into a PIL Image\n",
    "    pil_img = array_to_img(img)\n",
    "    # Convert into a cv2 image\n",
    "    cv_img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "    # Convert color space to HSV\n",
    "    hsv = cv2.cvtColor(cv_img, cv2.COLOR_BGR2HSV)\n",
    "    # Create mask by applying thresholds\n",
    "    mask = cv2.inRange(hsv, min_params, max_params)\n",
    "    i, j = np.where(mask)\n",
    "    indices=None\n",
    "    if any(i) or any(j):\n",
    "        indices = np.meshgrid(np.arange(min(i), max(i) + 1),\n",
    "                              np.arange(min(j), max(j) + 1),\n",
    "                              indexing='ij')\n",
    "    if indices:\n",
    "        cropped_image = cv_img[indices]\n",
    "    else:\n",
    "        cropped_image = cv_img\n",
    "    # Resize here !!\n",
    "    resized_image = cv2.resize(cropped_image, (224,224))\n",
    "    resized_image = np.array(resized_image,dtype=np.float64)\n",
    "    \n",
    "    #print(img.shape) #(224, 224, 3)\n",
    "    img = np.expand_dims(resized_image, axis=0)\n",
    "    #print(img.shape) #(1, 224, 224, 3)\n",
    "    # 'RGB'->'BGR'\n",
    "    img = img[:, :, :, ::-1]\n",
    "    # Zero-center by mean pixel\n",
    "    img[:, :, :, 0] -= 103.939\n",
    "    img[:, :, :, 1] -= 116.779\n",
    "    img[:, :, :, 2] -= 123.68\n",
    "    #img[:, :, :, 0] -= 78.939\n",
    "    #img[:, :, :, 1] -= 91.779\n",
    "    #img[:, :, :, 2] -= 148.68\n",
    "    #print(img.shape)\n",
    "    #print(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# checkpoint\n",
    "# http://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "filen=\"VGG16D_1-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "filepath = os.path.join(weights_dir, filen)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "# !!! TRAIN\n",
    "#callbacks_list = [checkpoint, early]\n",
    "#callbacks_list= [checkpoint]\n",
    "callbacks_list=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_pk,\n",
    "                                horizontal_flip=True,\n",
    "                                vertical_flip=True,\n",
    "                                 channel_shift_range=5)\n",
    "        #rotation_range=20,\n",
    "        #width_shift_range=0.1,\n",
    "        #height_shift_range=0.1,\n",
    "        #shear_range=0.1,\n",
    "        #zoom_range=0.1,\n",
    "        #horizontal_flip=True,\n",
    "        #fill_mode='nearest')\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess)\n",
    "\n",
    "\n",
    "#Arguments\n",
    "#    x: input Numpy tensor, 4D.\n",
    "#    data_format: data format of the image tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 473 images belonging to 3 classes.\n",
      "Found 362 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    classes = ['Type_1', 'Type_2', 'Type_3'],\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    classes = ['Type_1', 'Type_2', 'Type_3'],\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this for preliminary timing on the AWS p2 instance, so can estimate a good size for the sample directory files\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !! SAMPLE\n",
    "#class_weight_dict = utils.get_class_weight_dict([35,106,62])\n",
    "# !! TRAIN\n",
    "class_weight_dict = utils.get_class_weight_dict([250,781,450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "8/8 [==============================] - 199s - loss: 16.8839 - acc: 0.3575 - val_loss: 10.0196 - val_acc: 0.3674\n",
      "Epoch 2/15\n",
      "8/8 [==============================] - 130s - loss: 16.1175 - acc: 0.4068 - val_loss: 9.6649 - val_acc: 0.3757\n",
      "Epoch 3/15\n",
      "8/8 [==============================] - 119s - loss: 15.9601 - acc: 0.4294 - val_loss: 10.5211 - val_acc: 0.3260\n",
      "Epoch 4/15\n",
      "8/8 [==============================] - 105s - loss: 17.2920 - acc: 0.3893 - val_loss: 9.6887 - val_acc: 0.3867\n",
      "Epoch 5/15\n",
      "8/8 [==============================] - 107s - loss: 17.2381 - acc: 0.3614 - val_loss: 10.7020 - val_acc: 0.3232\n",
      "Epoch 6/15\n",
      "8/8 [==============================] - 119s - loss: 17.1632 - acc: 0.3769 - val_loss: 10.4873 - val_acc: 0.3398\n",
      "Epoch 7/15\n",
      "8/8 [==============================] - 111s - loss: 16.4184 - acc: 0.4343 - val_loss: 9.9683 - val_acc: 0.3674\n",
      "Epoch 8/15\n",
      "8/8 [==============================] - 114s - loss: 16.3407 - acc: 0.4292 - val_loss: 10.7823 - val_acc: 0.3149\n",
      "Epoch 9/15\n",
      "7/8 [=========================>....] - ETA: 2s - loss: 16.3711 - acc: 0.4353"
     ]
    }
   ],
   "source": [
    "h = vgg16d_1.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps, \n",
    "    class_weight = class_weight_dict,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.optimizer.lr=0.0001 # Default 0.001\n",
    "h = vgg16d_1.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps, \n",
    "    class_weight = class_weight_dict,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use this for preliminary timing on the AWS p2 instance, so can estimate a good size for the sample directory files\n",
    "print(\"%f seconds\" % (time.time() - start_time))\n",
    "print('Total Images Processed ' , str(num_training_samples + num_validation_samples))\n",
    "print('Epochs :', str(h.epoch[-1]+ 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.plot_learning(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg16d_1.optimizer.lr=1e-4 # Lift the learning rate\n",
    "h = vgg16d_1.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps, \n",
    "    class_weight = class_weight_dict,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.plot_learning(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg16d_1.optimizer.lr=1e-5 # Seems to be just bouncing around, so put lr back to original\n",
    "h = vgg16d_1.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps, \n",
    "    class_weight = class_weight_dict,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1)\n",
    "utils.plot_learning(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg16d_1.optimizer.lr=1e-5 # \n",
    "h = vgg16d_1.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps, \n",
    "    class_weight = class_weight_dict,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1)\n",
    "utils.plot_learning(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = vgg16d_1.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps, \n",
    "    class_weight = class_weight_dict,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1)\n",
    "utils.plot_learning(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = vgg16d_1.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps, \n",
    "    class_weight = class_weight_dict,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1)\n",
    "utils.plot_learning(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16d_1.optimizer.lr=1e-6\n",
    "h = vgg16d_1.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps, \n",
    "    class_weight = class_weight_dict,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1)\n",
    "utils.plot_learning(h)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ana_py27_p2]",
   "language": "python",
   "name": "conda-env-ana_py27_p2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
