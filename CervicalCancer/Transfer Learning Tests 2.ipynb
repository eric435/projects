{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System : Python  2.7.12 | packaged by conda-forge | (default, Sep  8 2016, 14:22:31) \n",
      "[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] \n",
      "\n",
      "\n",
      "Directory Structure:\n",
      "/home/ubuntu/ana_py27_p2/projects/CervicalCancer\n",
      ".\n",
      "├── unused_utils\n",
      "│   ├── __init__.py\n",
      "│   ├── __init__.pyc\n",
      "│   ├── utils.pyc\n",
      "│   └── unused_utils.py\n",
      "├── __init__.py\n",
      "├── 001 - Initial Setup.ipynb\n",
      "├── 002 - Data Setup & General Notes.ipynb\n",
      "├── 003 - Baseline.ipynb\n",
      "├── Trash\n",
      "│   ├── 002-01 - Train Data (only) Setup.ipynb\n",
      "│   └── Base CNN Model Train Data with Preprocessing.ipynb\n",
      "├── 004-1 - Base CNN Model - Sample.ipynb\n",
      "├── 004-2 - Base CNN Model - Train Data.ipynb\n",
      "├── 005 - Exploratoring the Data.ipynb\n",
      "├── 006 - Non Neural Net Classifiers.ipynb\n",
      "├── 007 - Gradient Boosting Classifier.ipynb\n",
      "├── data\n",
      "│   ├── additional_Type_1_v2.7z\n",
      "│   ├── additional_Type_3_v2.7z\n",
      "│   ├── additional_Type_2_v2.7z\n",
      "│   ├── train\n",
      "│   │   ├── Type_1 [168 entries exceeds filelimit, not opening dir]\n",
      "│   │   ├── Type_2 [524 entries exceeds filelimit, not opening dir]\n",
      "│   │   └── Type_3 [302 entries exceeds filelimit, not opening dir]\n",
      "│   ├── test\n",
      "│   │   └── unknown [512 entries exceeds filelimit, not opening dir]\n",
      "│   ├── preview\n",
      "│   ├── downloads\n",
      "│   │   ├── sample_submission.csv.zip\n",
      "│   │   ├── test.7z\n",
      "│   │   └── train.7z\n",
      "│   ├── valid\n",
      "│   │   ├── Type_1 [82 entries exceeds filelimit, not opening dir]\n",
      "│   │   ├── Type_2 [257 entries exceeds filelimit, not opening dir]\n",
      "│   │   └── Type_3 [148 entries exceeds filelimit, not opening dir]\n",
      "│   ├── sample\n",
      "│   │   ├── preview\n",
      "│   │   ├── train\n",
      "│   │   │   ├── Type_1 [80 entries exceeds filelimit, not opening dir]\n",
      "│   │   │   ├── Type_2 [249 entries exceeds filelimit, not opening dir]\n",
      "│   │   │   └── Type_3 [144 entries exceeds filelimit, not opening dir]\n",
      "│   │   ├── valid\n",
      "│   │   │   ├── Type_1 [61 entries exceeds filelimit, not opening dir]\n",
      "│   │   │   ├── Type_2 [191 entries exceeds filelimit, not opening dir]\n",
      "│   │   │   └── Type_3 [110 entries exceeds filelimit, not opening dir]\n",
      "│   │   ├── test\n",
      "│   │   │   └── unknown [201 entries exceeds filelimit, not opening dir]\n",
      "│   │   └── weights\n",
      "│   │       ├── weights-improvement-00-0.22.hdf5\n",
      "│   │       ├── weights-improvement-01-0.35.hdf5\n",
      "│   │       ├── weights-improvement-03-0.36.hdf5\n",
      "│   │       ├── weights-improvement-06-0.45.hdf5\n",
      "│   │       ├── weights-improvement-07-0.47.hdf5\n",
      "│   │       ├── BaseCNN35EpochRun_1.hdf5\n",
      "│   │       ├── weights2-improvement-00-0.30.hdf5\n",
      "│   │       ├── weights2-improvement-01-0.37.hdf5\n",
      "│   │       ├── weights2-improvement-05-0.43.hdf5\n",
      "│   │       ├── weights2-improvement-11-0.46.hdf5\n",
      "│   │       └── weights2-improvement-12-0.50.hdf5\n",
      "│   ├── submissions\n",
      "│   │   ├── sample_submission_000.csv\n",
      "│   │   ├── sub_001.csv\n",
      "│   │   ├── sub_002.csv\n",
      "│   │   ├── sub_003.csv\n",
      "│   │   ├── sub_004.csv\n",
      "│   │   └── sub_005.csv\n",
      "│   ├── weights [29 entries exceeds filelimit, not opening dir]\n",
      "│   └── models\n",
      "│       └── VGG16D_1.yaml\n",
      "├── Transfer Learning Tests 1.ipynb\n",
      "└── Transfer Learning Tests 2.ipynb\n",
      "\n",
      "31 directories, 41 files\n",
      "\n",
      "\n",
      "Keras version: 2.0.2 , backend: theano , image_format: channels_last\n",
      "\n",
      "\n",
      "Environment : \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='../../ana_py27_p2.yml' target='_blank'>../../ana_py27_p2.yml</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/ana_py27_p2/ana_py27_p2.yml"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project Template Import Cell\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from inspect import getsourcefile\n",
    "\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Standard Notebook Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "CURR_PATH = os.path.abspath(getsourcefile(lambda:0)).rsplit('/', 1)[0] # Get filepath of this notebook\n",
    "module_path = os.path.join(os.path.dirname(CURR_PATH), 'utils') # Make module path for one dir up and one down into utils\n",
    "if module_path not in sys.path: # Append to system path list\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import utils as utils ; reload(utils)\n",
    "\n",
    "print('System : Python ', os.sys.version , '\\n\\n')\n",
    "\n",
    "print('Directory Structure:')\n",
    "print(CURR_PATH)\n",
    "!tree -cn --filelimit 16\n",
    "\n",
    "# Keras Setup\n",
    "import keras\n",
    "print('\\n\\nKeras version:' , keras.__version__ ,\n",
    "      ', backend:' , keras.backend.backend(),\n",
    "      ', image_format:' , keras.backend.image_data_format())\n",
    "\n",
    "random_seed = 2\n",
    "\n",
    "print('\\n\\nEnvironment : ')\n",
    "FileLink('../../ana_py27_p2.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Strategy\n",
    "[Fine-tuning the top layers of a a pre-trained network](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) - I will attempt to follow the Final Part of this tutorial, but the final part build upon work done in Part 2, so I will start at Part 2 - Using the bottleneck features of a pre-trained network: 90% accuracy in a minute\n",
    "\n",
    "1. Load up VGG16 and its weights\n",
    "2. Chop off the top layer of VGG16, and add a small fully connected model\n",
    "3. Freeze all the layers in VGG16, and train the final fully connected model\n",
    "4. Unfreeze the last convolutional layer also, and retrain the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A fix for truncated image error\n",
    "# http://stackoverflow.com/questions/12984426/python-pil-ioerror-image-file-truncated-with-big-images\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# DIRECTORIES\n",
    "# !!!SAMPLE\n",
    "#train_data_dir = 'data/sample/train' ; validation_data_dir = 'data/sample/valid' ; test_data_dir = 'data/sample/test'\n",
    "#weights_dir = 'data/sample/weights'\n",
    "\n",
    "# !!!TRAIN\n",
    "train_data_dir = 'data/train' ; validation_data_dir = 'data/valid' ; test_data_dir = 'data/test'\n",
    "weights_dir = 'data/weights'\n",
    "\n",
    "submission_dir = 'data/submissions'\n",
    "pt_models_dir = '../pretrained_models'\n",
    "models_dir = 'data/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n",
      "487\n",
      "16\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# DATA META-DATA\n",
    "# Get the DataSets and load into a DataFrame for easy reference\n",
    "\n",
    "img_width, img_height = 224, 224\n",
    "#img_width, img_height = 300, 300\n",
    "\n",
    "num_training_samples = 0\n",
    "for class_dir in utils.get_non_hidden_dir_contents(train_data_dir):\n",
    "    num_training_samples += len(utils.get_non_hidden_dir_contents(class_dir))\n",
    "num_validation_samples = 0\n",
    "for class_dir in utils.get_non_hidden_dir_contents(validation_data_dir):\n",
    "    num_validation_samples += len(utils.get_non_hidden_dir_contents(class_dir))    \n",
    "\n",
    "print(num_training_samples)\n",
    "print(num_validation_samples)\n",
    "\n",
    "# !!!SAMPLE\n",
    "#epochs = 120\n",
    "#batch_size=25\n",
    "\n",
    "# !!!TRAIN\n",
    "epochs=30\n",
    "batch_size=64\n",
    "\n",
    "input_shape = (img_width, img_height, 3)\n",
    "\n",
    "train_steps = int(math.ceil(num_training_samples / batch_size))\n",
    "validation_steps =  int(math.ceil(num_validation_samples / batch_size))\n",
    "print(train_steps)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load up VGG16D architecture & saved best weights from last model runs and Unfreeze the Dense and last Conv block\n",
    "\n",
    "from keras.models import model_from_yaml\n",
    "import yaml\n",
    "\n",
    "# Load Model VGG 16D - VGG16 + Dense top model\n",
    "yaml_file = open(os.path.join(models_dir, 'vgg16model_1.yaml'), 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "vgg16d_1 = model_from_yaml(loaded_model_yaml)\n",
    "\n",
    "vgg16d_1 = Model(inputs=[vgg16d_1.input], outputs=[vgg16d_1.layers[-5].output])\n",
    "\n",
    "# load best weights from previously trained model (Everything frozen apart from top Dense model)\n",
    "vgg16d_1.load_weights(os.path.join(weights_dir,'VGG16D_1_2-improvement-06-0.56.hdf5'))\n",
    "print(\"Loaded weights from disk\")\n",
    "#Make top Dense and top Convolution Block Trainable\n",
    "#for l in vgg16d_1.layers[:]:\n",
    "#    l.trainable = False\n",
    "for l in vgg16d_1.layers[14:]:\n",
    "    l.trainable = True\n",
    "vgg16d_1.summary() # Trainable parameters about 13,502,979 i.e 13 million compared to the original 138 million\n",
    "# We can actually only train the weights in the final convolution block, and in the final dense layer\n",
    "print(vgg16d_1.layers[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from disk\n",
      "[<keras.engine.topology.InputLayer object at 0x7fe3f30ca190>, <keras.layers.convolutional.Conv2D object at 0x7fe3f2f52e50>, <keras.layers.convolutional.Conv2D object at 0x7fe3f2f52f10>, <keras.layers.pooling.MaxPooling2D object at 0x7fe3f1da46d0>, <keras.layers.convolutional.Conv2D object at 0x7fe3f7759dd0>, <keras.layers.convolutional.Conv2D object at 0x7fe3f1d18510>, <keras.layers.pooling.MaxPooling2D object at 0x7fe3f1c98e90>, <keras.layers.convolutional.Conv2D object at 0x7fe3f1bbed90>, <keras.layers.convolutional.Conv2D object at 0x7fe3f1bbe090>, <keras.layers.convolutional.Conv2D object at 0x7fe3f1b48c90>, <keras.layers.pooling.MaxPooling2D object at 0x7fe3f1addf10>, <keras.layers.convolutional.Conv2D object at 0x7fe3f1a5a9d0>, <keras.layers.convolutional.Conv2D object at 0x7fe3f1a5a090>, <keras.layers.convolutional.Conv2D object at 0x7fe3f3953590>, <keras.layers.pooling.MaxPooling2D object at 0x7fe3f1963d90>, <keras.layers.convolutional.Conv2D object at 0x7fe3f18e96d0>, <keras.layers.convolutional.Conv2D object at 0x7fe3f186f2d0>, <keras.layers.convolutional.Conv2D object at 0x7fe3f17e8690>, <keras.layers.pooling.MaxPooling2D object at 0x7fe3f17898d0>, <keras.layers.core.Flatten object at 0x7fe3f2f15d10>, <keras.layers.core.Dense object at 0x7fe3f17fab50>, <keras.layers.core.Dropout object at 0x7fe3f2f15c10>, <keras.layers.core.Dense object at 0x7fe3f19a3590>]\n"
     ]
    }
   ],
   "source": [
    "# Load up VGG16 architecture & inspect\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Dropout\n",
    "from keras.models import model_from_yaml\n",
    "\n",
    "import yaml\n",
    "\n",
    "yaml_file = open(os.path.join(pt_models_dir, 'vgg16model_1.yaml'), 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "vgg16d_1 = model_from_yaml(loaded_model_yaml)\n",
    "\n",
    "#vgg16.summary()\n",
    "# Chop off the last 4 layers\n",
    "#vgg16_p = Model(inputs=[vgg16.input], outputs=[vgg16.get_layer('block5_pool').output]) #Grab output by name\n",
    "vgg16d_1 = Model(inputs=[vgg16d_1.input], outputs=[vgg16d_1.layers[-5].output])\n",
    "#utils.show_ascii_model(vgg16_p)\n",
    "from keras import regularizers\n",
    "# Create a new Dense model to add to the end of vgg16\n",
    "x = Flatten()(vgg16d_1.layers[-1].output)\n",
    "#x = Dense(256, activation='relu') (x)\n",
    "\n",
    "x = Dense(256, activation='relu', kernel_regularizer= regularizers.l2(1.0)) (x)\n",
    "\n",
    "x = Dropout(0.75) (x)\n",
    "\n",
    "last = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# Add to the end of VGG16\n",
    "vgg16d_1 = Model(inputs=[vgg16d_1.input], outputs=[last])\n",
    "# load weights into new model\n",
    "vgg16d_1.load_weights(os.path.join(weights_dir,'VGG16D_1_2-improvement-06-0.56.hdf5'))\n",
    "print(\"Loaded weights from disk\")\n",
    "\n",
    "#utils.show_ascii_model(vgg16_)\n",
    "#Freeze all the weights in the original vgg16 layers before compile\n",
    "for l in vgg16d_1.layers:\n",
    "    l.trainable=False\n",
    "for l in vgg16d_1.layers[14:]:\n",
    "    l.trainable = True\n",
    "#vgg16d_1.summary() # Trainable parameters about 13,502,979 i.e 13 million compared to the original 138 million\n",
    "# We can actually only train the weights in the final convolution block, and in the final dense block\n",
    "print(vgg16d_1.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-30d9529ff51c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg16d_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://github.com/Lasagne/Recipes/issues/20\n",
    "# https://stats.stackexchange.com/questions/217481/how-to-normalize-data-for-vgg-16-pretrained-model\n",
    "def preprocess(img):\n",
    "    \"\"\"\n",
    "    I don't get this - I thought this took an individual image\n",
    "    I am missing a dimension somewhere\n",
    "    Does it take a batch of images?? - I don't think so according to docs\n",
    "    ???????????????????\n",
    "    \"\"\"\n",
    "    #print(img.shape) #(224, 224, 3)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    #print(img.shape) #(1, 224, 224, 3)\n",
    "    # 'RGB'->'BGR'\n",
    "    img = img[:, :, :, ::-1]\n",
    "    # Zero-center by mean pixel\n",
    "    img[:, :, :, 0] -= 103.939\n",
    "    img[:, :, :, 1] -= 116.779\n",
    "    img[:, :, :, 2] -= 123.68\n",
    "    #print(img.shape)\n",
    "    #print(img)\n",
    "    return img\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(type(img))\n",
    "    #to_bw = np.array([0.299, 0.587, 0.114])\n",
    "\n",
    "    #if keras.backend.image_data_format() =='channels_last':\n",
    "        #x = np.rollaxis(img, 0, 3).dot(to_bw)\n",
    "        #print(x.shape)\n",
    "        #return x\n",
    "        #ValueError: shapes (224,3,224) and (3,) not aligned: 224 (dim 2) != 3 (dim 0)    \n",
    "\n",
    "    # 'RGB'->'BGR'\n",
    "    #img = img[:, :, ::-1]\n",
    "    #print(img.shape)\n",
    "     #Zero-center by mean pixel\n",
    "    #img[:, :, 0] -= 103.939\n",
    "    #img[:, :, 1] -= 116.779\n",
    "    #img[:, :, 2] -= 123.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# checkpoint\n",
    "# http://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "filen=\"VGG16D_1-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "filepath = os.path.join(weights_dir, filen)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "# !!! TRAIN\n",
    "#callbacks_list = [checkpoint, early]\n",
    "#callbacks_list= [checkpoint]\n",
    "callbacks_list=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess)\n",
    "\n",
    "\n",
    "#Arguments\n",
    "#    x: input Numpy tensor, 4D.\n",
    "#    data_format: data format of the image tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    classes = ['Type_1', 'Type_2', 'Type_3'],\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    classes = ['Type_1', 'Type_2', 'Type_3'],\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this for preliminary timing on the AWS p2 instance, so can estimate a good size for the sample directory files\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !! SAMPLE\n",
    "#class_weight_dict = utils.get_class_weight_dict([35,106,62])\n",
    "# !! TRAIN\n",
    "class_weight_dict = utils.get_class_weight_dict([250,781,450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = vgg16d_1.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps, \n",
    "    class_weight = class_weight_dict,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use this for preliminary timing on the AWS p2 instance, so can estimate a good size for the sample directory files\n",
    "print(\"%f seconds\" % (time.time() - start_time))\n",
    "print('Total Images Processed ' , str(num_training_samples + num_validation_samples))\n",
    "print('Epochs :', str(h.epoch[-1]+ 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.plot_learning(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ana_py27_p2]",
   "language": "python",
   "name": "conda-env-ana_py27_p2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
