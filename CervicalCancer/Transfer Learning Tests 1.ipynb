{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System : Python  2.7.12 | packaged by conda-forge | (default, Sep  8 2016, 14:22:31) \n",
      "[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] \n",
      "\n",
      "\n",
      "Directory Structure:\n",
      "/home/ubuntu/ana_py27_p2/projects/CervicalCancer\n",
      ".\n",
      "├── unused_utils\n",
      "│   ├── __init__.py\n",
      "│   ├── __init__.pyc\n",
      "│   ├── utils.pyc\n",
      "│   └── unused_utils.py\n",
      "├── __init__.py\n",
      "├── 001 - Initial Setup.ipynb\n",
      "├── 002 - Data Setup & General Notes.ipynb\n",
      "├── 003 - Baseline.ipynb\n",
      "├── Trash\n",
      "│   ├── 002-01 - Train Data (only) Setup.ipynb\n",
      "│   └── Base CNN Model Train Data with Preprocessing.ipynb\n",
      "├── 004-1 - Base CNN Model - Sample.ipynb\n",
      "├── 004-2 - Base CNN Model - Train Data.ipynb\n",
      "├── 005 - Exploratoring the Data.ipynb\n",
      "├── 006 - Non Neural Net Classifiers.ipynb\n",
      "├── 007 - Gradient Boosting Classifier.ipynb\n",
      "├── Transfer Learning Tests 2.ipynb\n",
      "├── data\n",
      "│   ├── additional_Type_1_v2.7z\n",
      "│   ├── additional_Type_3_v2.7z\n",
      "│   ├── additional_Type_2_v2.7z\n",
      "│   ├── train\n",
      "│   │   ├── Type_1 [168 entries exceeds filelimit, not opening dir]\n",
      "│   │   ├── Type_2 [524 entries exceeds filelimit, not opening dir]\n",
      "│   │   └── Type_3 [302 entries exceeds filelimit, not opening dir]\n",
      "│   ├── test\n",
      "│   │   └── unknown [512 entries exceeds filelimit, not opening dir]\n",
      "│   ├── preview\n",
      "│   ├── downloads\n",
      "│   │   ├── sample_submission.csv.zip\n",
      "│   │   ├── test.7z\n",
      "│   │   └── train.7z\n",
      "│   ├── valid\n",
      "│   │   ├── Type_1 [82 entries exceeds filelimit, not opening dir]\n",
      "│   │   ├── Type_2 [257 entries exceeds filelimit, not opening dir]\n",
      "│   │   └── Type_3 [148 entries exceeds filelimit, not opening dir]\n",
      "│   ├── sample\n",
      "│   │   ├── preview\n",
      "│   │   ├── train\n",
      "│   │   │   ├── Type_1 [80 entries exceeds filelimit, not opening dir]\n",
      "│   │   │   ├── Type_2 [249 entries exceeds filelimit, not opening dir]\n",
      "│   │   │   └── Type_3 [144 entries exceeds filelimit, not opening dir]\n",
      "│   │   ├── valid\n",
      "│   │   │   ├── Type_1 [61 entries exceeds filelimit, not opening dir]\n",
      "│   │   │   ├── Type_2 [191 entries exceeds filelimit, not opening dir]\n",
      "│   │   │   └── Type_3 [110 entries exceeds filelimit, not opening dir]\n",
      "│   │   ├── test\n",
      "│   │   │   └── unknown [201 entries exceeds filelimit, not opening dir]\n",
      "│   │   └── weights\n",
      "│   │       ├── weights-improvement-00-0.22.hdf5\n",
      "│   │       ├── weights-improvement-01-0.35.hdf5\n",
      "│   │       ├── weights-improvement-03-0.36.hdf5\n",
      "│   │       ├── weights-improvement-06-0.45.hdf5\n",
      "│   │       ├── weights-improvement-07-0.47.hdf5\n",
      "│   │       ├── BaseCNN35EpochRun_1.hdf5\n",
      "│   │       ├── weights2-improvement-00-0.30.hdf5\n",
      "│   │       ├── weights2-improvement-01-0.37.hdf5\n",
      "│   │       ├── weights2-improvement-05-0.43.hdf5\n",
      "│   │       ├── weights2-improvement-11-0.46.hdf5\n",
      "│   │       └── weights2-improvement-12-0.50.hdf5\n",
      "│   ├── submissions\n",
      "│   │   ├── sample_submission_000.csv\n",
      "│   │   ├── sub_001.csv\n",
      "│   │   ├── sub_002.csv\n",
      "│   │   ├── sub_003.csv\n",
      "│   │   ├── sub_004.csv\n",
      "│   │   └── sub_005.csv\n",
      "│   ├── weights [29 entries exceeds filelimit, not opening dir]\n",
      "│   └── models\n",
      "└── Transfer Learning Tests 1.ipynb\n",
      "\n",
      "31 directories, 40 files\n",
      "\n",
      "\n",
      "Keras version: 2.0.2 , backend: theano , image_format: channels_last\n",
      "\n",
      "\n",
      "Environment : \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='../../ana_py27_p2.yml' target='_blank'>../../ana_py27_p2.yml</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/ana_py27_p2/ana_py27_p2.yml"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project Template Import Cell\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from inspect import getsourcefile\n",
    "\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Standard Notebook Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "CURR_PATH = os.path.abspath(getsourcefile(lambda:0)).rsplit('/', 1)[0] # Get filepath of this notebook\n",
    "module_path = os.path.join(os.path.dirname(CURR_PATH), 'utils') # Make module path for one dir up and one down into utils\n",
    "if module_path not in sys.path: # Append to system path list\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import utils as utils ; reload(utils)\n",
    "\n",
    "print('System : Python ', os.sys.version , '\\n\\n')\n",
    "\n",
    "print('Directory Structure:')\n",
    "print(CURR_PATH)\n",
    "!tree -cn --filelimit 16\n",
    "\n",
    "# Keras Setup\n",
    "import keras\n",
    "print('\\n\\nKeras version:' , keras.__version__ ,\n",
    "      ', backend:' , keras.backend.backend(),\n",
    "      ', image_format:' , keras.backend.image_data_format())\n",
    "\n",
    "random_seed = 2\n",
    "\n",
    "print('\\n\\nEnvironment : ')\n",
    "FileLink('../../ana_py27_p2.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Strategy\n",
    "[Fine-tuning the top layers of a a pre-trained network](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) - I will attempt to follow the Final Part of this tutorial, but the final part build upon work done in Part 2, so I will start at Part 2 - Using the bottleneck features of a pre-trained network: 90% accuracy in a minute\n",
    "\n",
    "1. Load up VGG16 and its weights\n",
    "2. Chop off the top layer of VGG16, and add a small fully connected model\n",
    "3. Freeze all the layers in VGG16, and train the final fully connected model\n",
    "4. Unfreeze the last convolutional layer also, and retrain the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A fix for truncated image error\n",
    "# http://stackoverflow.com/questions/12984426/python-pil-ioerror-image-file-truncated-with-big-images\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# DIRECTORIES\n",
    "# !!!SAMPLE\n",
    "#train_data_dir = 'data/sample/train' ; validation_data_dir = 'data/sample/valid' ; test_data_dir = 'data/sample/test'\n",
    "#weights_dir = 'data/sample/weights'\n",
    "\n",
    "# !!!TRAIN\n",
    "train_data_dir = 'data/train' ; validation_data_dir = 'data/valid' ; test_data_dir = 'data/test'\n",
    "weights_dir = 'data/weights'\n",
    "\n",
    "submission_dir = 'data/submissions'\n",
    "pt_models_dir = '../pretrained_models'\n",
    "models_dir = 'data/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n",
      "487\n",
      "16\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# DATA META-DATA\n",
    "# Get the DataSets and load into a DataFrame for easy reference\n",
    "\n",
    "img_width, img_height = 224, 224\n",
    "#img_width, img_height = 300, 300\n",
    "\n",
    "num_training_samples = 0\n",
    "for class_dir in utils.get_non_hidden_dir_contents(train_data_dir):\n",
    "    num_training_samples += len(utils.get_non_hidden_dir_contents(class_dir))\n",
    "num_validation_samples = 0\n",
    "for class_dir in utils.get_non_hidden_dir_contents(validation_data_dir):\n",
    "    num_validation_samples += len(utils.get_non_hidden_dir_contents(class_dir))    \n",
    "\n",
    "print(num_training_samples)\n",
    "print(num_validation_samples)\n",
    "\n",
    "# !!!SAMPLE\n",
    "#epochs = 120\n",
    "#batch_size=25\n",
    "\n",
    "# !!!TRAIN\n",
    "epochs=20\n",
    "batch_size=64\n",
    "\n",
    "input_shape = (img_width, img_height, 3)\n",
    "\n",
    "train_steps = int(math.ceil(num_training_samples / batch_size))\n",
    "validation_steps =  int(math.ceil(num_validation_samples / batch_size))\n",
    "print(train_steps)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from disk\n"
     ]
    }
   ],
   "source": [
    "# Load up VGG16 architecture & inspect\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Dropout\n",
    "from keras.models import model_from_yaml\n",
    "\n",
    "import yaml\n",
    "\n",
    "yaml_file = open(os.path.join(pt_models_dir, 'vgg16model_1.yaml'), 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "vgg16 = model_from_yaml(loaded_model_yaml)\n",
    "\n",
    "# load weights into new model\n",
    "vgg16.load_weights(os.path.join(pt_models_dir,'vgg16weights_1.hdf5'))\n",
    "print(\"Loaded weights from disk\")\n",
    "\n",
    "#vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Chop off the last 4 layers\n",
    "#vgg16_p = Model(inputs=[vgg16.input], outputs=[vgg16.get_layer('block5_pool').output]) #Grab output by name\n",
    "vgg16_p = Model(inputs=[vgg16.input], outputs=[vgg16.layers[-5].output])\n",
    "#utils.show_ascii_model(vgg16_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "# Create a new Dense model to add to the end of vgg16\n",
    "x = Flatten()(vgg16_p.layers[-1].output)\n",
    "#x = Dense(256, activation='relu') (x)\n",
    "x = Dense(256, activation='relu', kernel_regularizer= regularizers.l2(1.0)) (x)\n",
    "x = Dropout(0.75) (x)\n",
    "last = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# Add to the end of VGG16\n",
    "vgg16_ = Model(inputs=[vgg16_p.input], outputs=[last])\n",
    "#utils.show_ascii_model(vgg16_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 21,138,243.0\n",
      "Trainable params: 6,423,555.0\n",
      "Non-trainable params: 14,714,688.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Freeze all the weights in the original vgg16 layers before compile\n",
    "for l in vgg16_.layers[0:-4]:\n",
    "    l.trainable = False\n",
    "vgg16_.summary() # Trainable parameters about 6 million compared to the original 138 million\n",
    "# We can actually only train the weights in the 2 final dense layers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convert to Code to save model\n",
    "# Saving architecture only (no weights)\n",
    "vgg16_1_yaml = vgg16_.to_yaml()\n",
    "with open(os.path.join(models_dir, 'VGG16D_1.yaml'), 'w') as yaml_file:\n",
    "    yaml_file.write(vgg16_1_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-30d9529ff51c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "#stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# checkpoint\n",
    "# http://machinelearningmastery.com/check-point-deep-learning-models-keras/ VGG16D_1\n",
    "filen=\"VGG16D_1_2-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "filepath = os.path.join(weights_dir, filen)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "# !!! TRAIN\n",
    "#callbacks_list = [checkpoint, early]\n",
    "callbacks_list= [checkpoint]\n",
    "#callbacks_list=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg16_.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://github.com/Lasagne/Recipes/issues/20\n",
    "# https://stats.stackexchange.com/questions/217481/how-to-normalize-data-for-vgg-16-pretrained-model\n",
    "def preprocess(img):\n",
    "    \"\"\"\n",
    "    I don't get this - I thought this took an individual image\n",
    "    I am missing a dimension somewhere\n",
    "    Does it take a batch of images?? - I don't think so according to docs\n",
    "    ???????????????????\n",
    "    \"\"\"\n",
    "    #print(img.shape) #(224, 224, 3)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    #print(img.shape) #(1, 224, 224, 3)\n",
    "    # 'RGB'->'BGR'\n",
    "    img = img[:, :, :, ::-1]\n",
    "    # Zero-center by mean pixel\n",
    "    img[:, :, :, 0] -= 103.939\n",
    "    img[:, :, :, 1] -= 116.779\n",
    "    img[:, :, :, 2] -= 123.68\n",
    "    #print(img.shape)\n",
    "    #print(img)\n",
    "    return img\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(type(img))\n",
    "    #to_bw = np.array([0.299, 0.587, 0.114])\n",
    "\n",
    "    #if keras.backend.image_data_format() =='channels_last':\n",
    "        #x = np.rollaxis(img, 0, 3).dot(to_bw)\n",
    "        #print(x.shape)\n",
    "        #return x\n",
    "        #ValueError: shapes (224,3,224) and (3,) not aligned: 224 (dim 2) != 3 (dim 0)    \n",
    "\n",
    "    # 'RGB'->'BGR'\n",
    "    #img = img[:, :, ::-1]\n",
    "    #print(img.shape)\n",
    "     #Zero-center by mean pixel\n",
    "    #img[:, :, 0] -= 103.939\n",
    "    #img[:, :, 1] -= 116.779\n",
    "    #img[:, :, 2] -= 123.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess)\n",
    "\n",
    "\n",
    "#Arguments\n",
    "#    x: input Numpy tensor, 4D.\n",
    "#    data_format: data format of the image tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    classes = ['Type_1', 'Type_2', 'Type_3'],\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    classes = ['Type_1', 'Type_2', 'Type_3'],\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this for preliminary timing on the AWS p2 instance, so can estimate a good size for the sample directory files\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !! SAMPLE\n",
    "#class_weight_dict = utils.get_class_weight_dict([35,106,62])\n",
    "# !! TRAIN\n",
    "class_weight_dict = utils.get_class_weight_dict([250,781,450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = vgg16_.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps, \n",
    "    class_weight = class_weight_dict,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note - Best saved weights at:\n",
    "# VGG16D_1_2-improvement-06-0.56.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use this for preliminary timing on the AWS p2 instance, so can estimate a good size for the sample directory files\n",
    "print(\"%f seconds\" % (time.time() - start_time))\n",
    "print('Total Images Processed ' , str(num_training_samples + num_validation_samples))\n",
    "print('Epochs :', str(h.epoch[-1]+ 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.plot_learning(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preparing pink cropper for use in Network - preprocess_pk\n",
    "# This may have problems because of the mean subtraction that takes place (part of VGG16 requirements)\n",
    "\n",
    "import cv2\n",
    "import PIL\n",
    "#import PIL.Image\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "def preprocess_pk(img):\n",
    "    \"\"\"\n",
    "    I don't get this - I thought this took an individual image\n",
    "    I am missing a dimension somewhere\n",
    "    Does it take a batch of images?? - I don't think so according to docs\n",
    "    ???????????????????\n",
    "    \"\"\"\n",
    "    # Set parameters\n",
    "    min_params = (130,85,150) ; max_params = (180,256,256)\n",
    "    # Convert back into a PIL Image\n",
    "    pil_img = array_to_img(img)\n",
    "    # Convert into a cv2 image\n",
    "    cv_img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "    # Convert color space to HSV\n",
    "    hsv = cv2.cvtColor(cv_img, cv2.COLOR_BGR2HSV)\n",
    "    # Create mask by applying thresholds\n",
    "    mask = cv2.inRange(hsv, min_params, max_params)\n",
    "    i, j = np.where(mask)\n",
    "    indices=None\n",
    "    if any(i) or any(j):\n",
    "        indices = np.meshgrid(np.arange(min(i), max(i) + 1),\n",
    "                              np.arange(min(j), max(j) + 1),\n",
    "                              indexing='ij')\n",
    "    if indices:\n",
    "        cropped_image = cv_img[indices]\n",
    "    else:\n",
    "        cropped_image = cv_img\n",
    "    # Resize here !!\n",
    "    resized_image = cv2.resize(cropped_image, (224,224))\n",
    "    \n",
    "    #print(img.shape) #(224, 224, 3)\n",
    "    img = np.expand_dims(resized_image, axis=0)\n",
    "    #print(img.shape) #(1, 224, 224, 3)\n",
    "    # 'RGB'->'BGR'\n",
    "    img = img[:, :, :, ::-1]\n",
    "    # Zero-center by mean pixel\n",
    "    img[:, :, :, 0] -= 103.939\n",
    "    img[:, :, :, 1] -= 116.779\n",
    "    img[:, :, :, 2] -= 123.68\n",
    "    #print(img.shape)\n",
    "    #print(img)\n",
    "    return img"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ana_py27_p2]",
   "language": "python",
   "name": "conda-env-ana_py27_p2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
